{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl-radiographs/data/labels.json\n",
      "('gcjgfx', 'json')\n",
      "1022\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from os import listdir\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_path = \"dl-radiographs/data/labels.json\"\n",
    "# Instantiate the parser\n",
    "# parser = argparse.ArgumentParser(description='Get data cleaning arguments')\n",
    "\n",
    "# parser.add_argument('data_path', type=str,\n",
    "#                     help='Relative source data path, including file name')\n",
    "\n",
    "# parser.add_argument('out_path', type=str,\n",
    "#                     help='Relative data output path, including file name')\n",
    "\n",
    "#Parse\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "#Clean data\n",
    "\n",
    "# data_path = args.data_path\n",
    "# out_path = args.out_path\n",
    "\n",
    "print(data_path)\n",
    "# print(out_path)\n",
    "\n",
    "\n",
    "if \".\" in data_path:\n",
    "    print(\"gcjgfx\", data_path.split(\".\")[-1])\n",
    "    if data_path.split(\".\")[-1] != \"json\":\n",
    "        raise ValueError(\"Invalid Extension\")\n",
    "else:\n",
    "    data_path + \".json\"\n",
    "\n",
    "\n",
    "# if \".\" in out_path:\n",
    "# \tprint(\"qwerty\", out_path.split(\".\")[-1])\n",
    "# \tif out_path.split(\".\")[-1] != \"csv\" and out_path.split(\".\")[-1] != \"txt\":\n",
    "# \t\traise ValueError(\"Invalid Extension\")\n",
    "# else:\n",
    "# \tout_path + \".csv\"\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "for line in open(data_path, 'r'):\n",
    "    data.append(json.loads(line))\t\n",
    "\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'B0': 1,\n",
       " u'Nproton': 10000000.0,\n",
       " u'dxyz': [0.491777037699935, 0.2885450477187051, 0.8215212113074664],\n",
       " u'iteration': 1,\n",
       " u'nb_blobs': 1,\n",
       " u'r': 1,\n",
       " u'run': 1,\n",
       " u'sign': -1,\n",
       " u'xyz': [2.16778017389989, 0.07290587000801096, 0.008047227368751853]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1022,)\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "number_blobs = []\n",
    "for dic in data:\n",
    "    number_blobs.append(dic['nb_blobs'])\n",
    "print(np.shape(number_blobs))\n",
    "Y = np.asarray(number_blobs)\n",
    "print(type(Y))\n",
    "Y -= 1 ## -1 since the labels are between 0 and 4 for softmax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1021, 691, 691, 3)\n",
      "14.4073040485\n"
     ]
    }
   ],
   "source": [
    "def get_img_list(path_to_dir, extension=\".png\"):\n",
    "\t\"\"\"\n",
    "\tCollects the filenames of all the files in a directory with a certaine extension\n",
    "\tInput: \n",
    "\tpath_to_dir: path to the directory\n",
    "\textension\n",
    "\tOutput:\n",
    "\tlist of file names\n",
    "\t\"\"\"\n",
    "\tfilenames = listdir(path_to_dir)\n",
    "\treturn [filename for filename in filenames if filename.endswith(extension)]\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "path_data = \"./project2/dl-radiographs/data/radiographs/\"\n",
    "\n",
    "#Get file lists\n",
    "data_list = get_img_list(path_data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tX = np.array([ndimage.imread(path_data+img_name, flatten=False) for img_name in data_list])\n",
    "\tprint X.shape\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape X: ', (1021, 691, 691, 3))\n"
     ]
    }
   ],
   "source": [
    "print('Shape X: ',np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 690.5, 690.5, -0.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQKBRACDQKAAQaBQACDQKAIS/AS789b+U9rLNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108161110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[10].astype('uint8'))\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 4 4 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape X_train: ', (911, 691, 691, 3))\n",
      "('Shape X_test: ', (110, 691, 691, 3))\n",
      "('Shape Y_train: ', (911,))\n",
      "('Shape Y_test: ', (110,))\n"
     ]
    }
   ],
   "source": [
    "indices = np.random.permutation(X.shape[0])\n",
    "training_idx, test_idx = indices[:110], indices[110:]\n",
    "X_test, X_train = X[training_idx,:], X[test_idx,:]\n",
    "Y_test, Y_train = Y[training_idx], Y[test_idx]\n",
    "print('Shape X_train: ', np.shape(X_train))\n",
    "print('Shape X_test: ', np.shape(X_test))\n",
    "print('Shape Y_train: ', np.shape(Y_train))\n",
    "print('Shape Y_test: ', np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 3 0 4 3 2 4 3 4 3 4 2 2 0 2 1 2 1 1 0 4 0 0 4 0 1 2 4 0 0 0 2 1 1 2\n",
      " 2 3 3 0 3 2 0 3 0 0 4 3 3 1 2 0 1 4 4 1 1 4 0 0 0 4 0 0 0 0 2 1 1 0 3 2 2\n",
      " 3 2 1 2 0 0 2 4 1 2 4 3 3 2 4 3 4 1 2 0 4 0 1 1 0 3 4 3 0 1 0 1 0 1 2 0 3\n",
      " 1 0 2 1 2 3 2 4 2 2 3 4 3 2 1 4 4 0 2 2 1 4 0 3 0 0 4 1 0 4 4 2 1 4 1 0 3\n",
      " 1 2 0 1 1 0 4 2 3 4 1 0 3 4 3 0 3 1 2 0 2 2 4 0 1 1 1 1 4 0 2 0 0 3 0 0 2\n",
      " 4 1 4 4 0 1 0 3 0 4 2 4 2 4 4 2 3 3 0 3 2 1 2 2 0 0 1 0 4 0 0 2 0 1 3 2 1\n",
      " 3 1 2 0 4 4 0 4 0 1 2 1 4 0 0 0 2 3 1 1 1 0 4 3 2 1 3 4 1 2 2 2 3 1 0 3 3\n",
      " 2 0 4 4 2 3 1 0 4 3 1 0 2 3 2 0 3 3 4 1 0 4 0 4 2 1 0 0 3 1 4 4 0 1 3 2 2\n",
      " 0 2 1 1 3 1 0 1 1 3 1 0 3 1 2 0 4 3 2 2 0 2 2 4 3 3 4 3 0 2 1 1 1 3 0 2 0\n",
      " 0 1 1 3 0 0 4 0 0 0 3 2 2 3 1 2 2 0 2 0 4 1 2 2 1 3 4 1 1 2 0 2 2 0 4 3 1\n",
      " 0 3 4 0 0 3 1 1 0 0 0 1 2 4 1 2 2 4 0 1 1 4 1 0 0 1 0 1 4 3 0 2 2 2 0 3 0\n",
      " 0 2 2 2 1 3 0 2 1 2 2 0 4 0 0 1 4 3 0 0 2 3 2 4 3 1 0 4 0 2 4 2 4 2 2 0 3\n",
      " 1 3 1 4 3 0 2 0 0 2 3 2 4 3 3 2 2 4 4 4 3 0 3 3 3 3 4 0 3 0 3 2 3 4 2 1 1\n",
      " 4 0 0 4 3 3 4 4 4 3 1 1 2 2 3 4 2 4 0 2 4 0 2 0 4 2 0 3 4 3 3 3 1 0 0 4 4\n",
      " 3 2 0 3 0 2 4 2 3 3 0 1 1 3 0 0 0 0 0 3 1 4 1 3 0 3 2 4 3 3 0 4 0 3 4 1 1\n",
      " 0 3 1 0 1 4 0 0 0 3 2 1 0 2 0 2 0 3 0 4 1 2 0 2 2 2 2 3 1 4 1 4 1 0 2 2 1\n",
      " 2 4 1 2 4 2 2 3 4 4 0 3 1 2 0 3 4 3 0 1 0 1 1 0 1 2 1 4 0 2 1 0 3 1 2 1 4\n",
      " 0 1 0 3 2 1 0 2 1 1 4 1 2 4 2 1 2 2 4 1 1 2 4 3 1 2 0 0 3 1 0 4 4 1 2 2 1\n",
      " 0 1 4 3 2 4 1 3 3 4 0 1 1 1 4 4 1 1 1 3 0 3 1 3 0 1 0 3 4 3 3 4 2 3 0 0 2\n",
      " 1 1 4 1 1 3 1 2 4 1 3 2 0 2 1 2 4 2 2 4 0 0 4 4 2 0 0 4 3 2 2 1 2 2 0 1 1\n",
      " 1 4 3 3 0 3 1 4 4 2 2 0 3 0 4 0 2 0 3 2 2 2 1 3 1 1 2 0 0 4 2 4 0 3 3 1 4\n",
      " 3 4 2 1 4 3 1 4 2 3 0 0 1 0 4 3 3 3 4 1 2 0 4 2 4 2 4 2 4 0 4 1 4 1 2 0 0\n",
      " 3 0 2 2 0 1 0 1 3 3 1 2 4 0 3 4 0 0 4 3 4 0 3 4 0 1 0 1 4 0 0 1 1 2 3 3 1\n",
      " 4 1 1 4 4 1 3 3 0 0 1 0 3 3 1 3 0 2 2 2 4 1 2 0 3 2 3 0 2 4 3 3 2 1 4 2 1\n",
      " 1 2 3 3 1 0 1 0 1 3 3 4 1 3 4 1 0 3 2 4 4 2 4]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"batchnorm/add_1:0\", shape=(?, 346, 346, 32), dtype=float32)\n",
      "Tensor(\"batchnorm_1/add_1:0\", shape=(?, 173, 173, 64), dtype=float32)\n",
      "Tensor(\"batchnorm_2/add_1:0\", shape=(?, 87, 87, 128), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 43, 43, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Feel free to play with this cell\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, 691, 691, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "#  [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]   \n",
    "    # First convolutional layer\n",
    "    W_conv1 = tf.get_variable('W_conv1', dtype = tf.float32, shape = (5, 5, 3, 32), initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv1 = tf.get_variable('b_conv1', dtype = tf.float32, shape = (32, ), initializer = tf.zeros_initializer())\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    W_conv2 = tf.get_variable('W_conv2', dtype = tf.float32, shape = (3, 3, 32, 64), initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv2 = tf.get_variable('b_conv2', dtype = tf.float32, shape = (64, ), initializer = tf.zeros_initializer())\n",
    "\n",
    "    W_conv3 = tf.get_variable('W_conv3', dtype = tf.float32, shape = (3, 3, 64, 128), initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_conv3 = tf.get_variable('b_conv3', dtype = tf.float32, shape = (128, ), initializer = tf.zeros_initializer())\n",
    "\n",
    "    \n",
    "    W1 = tf.get_variable('W1', dtype = tf.float32, shape = (236672, 5), initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable('b1', dtype = tf.float32, shape = (5, ), initializer = tf.zeros_initializer())\n",
    "    \n",
    "#     W2 = tf.get_variable('W3', dtype = tf.float32, shape = (1024, 10), initializer = tf.contrib.layers.xavier_initializer())\n",
    "#     b2 = tf.get_variable('b3', dtype = tf.float32, shape = (10,), initializer = tf.zeros_initializer())\n",
    "    \n",
    "    \n",
    "    gamma1 = tf.get_variable('gamma1', dtype = tf.float32, shape = (32, ), initializer = tf.zeros_initializer())\n",
    "    beta1 = tf.get_variable('beta1', dtype = tf.float32, shape = (32, ), initializer = tf.ones_initializer())\n",
    "    \n",
    "    gamma2 = tf.get_variable('gamma2', dtype = tf.float32, shape = (64, ), initializer = tf.zeros_initializer())\n",
    "    beta2 = tf.get_variable('beta2', dtype = tf.float32, shape = (64, ), initializer = tf.ones_initializer())\n",
    "\n",
    "    gamma3 = tf.get_variable('gamma3', dtype = tf.float32, shape = (128, ), initializer = tf.zeros_initializer())\n",
    "    beta3 = tf.get_variable('beta3', dtype = tf.float32, shape = (128, ), initializer = tf.ones_initializer())\n",
    "\n",
    "    conv1 = tf.nn.conv2d(X, W_conv1, strides = [1, 2, 2, 1], padding = 'SAME') + b_conv1\n",
    "    r1 = tf.nn.relu(conv1)\n",
    "    mean1, variance1 = tf.nn.moments(r1, axes = [0, 1, 2])\n",
    "    bn1 = tf.nn.batch_normalization(r1, mean1, variance1, gamma1, beta1, 1e-8)\n",
    "    print(bn1)\n",
    "    \n",
    "    conv2 = tf.nn.conv2d(bn1, W_conv2, strides = [1, 2, 2, 1], padding = 'SAME') + b_conv2\n",
    "    r2 = tf.nn.relu(conv2)\n",
    "    mean2, variance2 = tf.nn.moments(r2, axes = [0, 1, 2])\n",
    "    bn2 = tf.nn.batch_normalization(r2, mean2, variance2, gamma2, beta2, 1e-8)  \n",
    "    print(bn2)\n",
    "    \n",
    "    conv3 = tf.nn.conv2d(bn2, W_conv3, strides = [1, 2, 2, 1], padding = 'SAME') + b_conv3\n",
    "    r3 = tf.nn.relu(conv3)\n",
    "    mean3, variance3 = tf.nn.moments(r3, axes = [0, 1, 2])\n",
    "    bn3 = tf.nn.batch_normalization(r3, mean3, variance3, gamma3, beta3, 1e-8)  \n",
    "    print(bn3)\n",
    "    mp3 = tf.nn.max_pool(bn3, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'VALID')\n",
    "    print(mp3)\n",
    "#     ll1 = tf.contrib.layers.fully_connected(mp2, 5, weights_initializer = tf.contrib.layers.xavier_initializer(), \n",
    "#                                             biases_initializer = tf.zeros_initializer())\n",
    "    \n",
    "    mp3 = tf.reshape(mp3, (-1, 236672))\n",
    "    ll1 = tf.matmul(mp3, W1) + b1\n",
    "    r4 = tf.nn.relu(ll1)\n",
    "#     ll2 = tf.matmul(r4, W2) + b2\n",
    "#     r5 = tf.nn.relu(ll2)\n",
    "#     ll3 = tf.matmul(r5, W3) + b3\n",
    "    reg = tf.nn.l2_loss(W_conv1) + tf.nn.l2_loss(W_conv2)\n",
    "    return r4, reg\n",
    "\n",
    "y_pred, reg = my_model(X,y,is_training)\n",
    "reg_param = 5e-3\n",
    "# total_loss = tf.nn.l2_loss(y_pred-y) + 0.5*reg_param*reg\n",
    "total_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y_pred, labels = y) + 0.5*reg_param*reg\n",
    "mean_loss = tf.reduce_mean(total_loss) \n",
    "# global_step = tf.Variable(0, trainable = False)\n",
    "starter_learning_rate = 1e-3\n",
    "# # lr = tf.train.exponential_decay(starter_learning_rate, global_step, 5000, 0.96)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 1e-3)\n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=10,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indices\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%X_train.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "           \n",
    "            print(np.shape(yd[idx]))\n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[i:i+batch_size].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss = session.run(variables,feed_dict=feed_dict)\n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "#             correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "#             if training_now and (iter_cnt % print_every) == 0:\n",
    "#                 print(iter_cnt,np.sum(corr)/float(actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/float(Xd.shape[0])\n",
    "        print(float(Xd.shape[0]))\n",
    "        total_loss = np.sum(losses)/float(Xd.shape[0])\n",
    "        print(total_loss,total_correct,e+1)\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Training')\n",
    "        run_model(sess,y_pred,mean_loss,X_train,Y_train,1,64,100,train_step,True)\n",
    "        print('Validation')\n",
    "        run_model(sess,y_pred,mean_loss,X_val,Y_val,1,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
